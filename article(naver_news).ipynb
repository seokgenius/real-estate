{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기사정보를 저장하는 함수\n",
    "def article_save(query1, query2, query3, ds, de):\n",
    "    # 기사제목, 링크주소, 좋아요, 화나요를 저장할 딕셔너리\n",
    "    article_dict = { 'title':[], 'link':[], 'like':[], 'angry':[] }\n",
    "    \n",
    "    # article_search()에서 입력한 조건으로 기사를 검색한다.\n",
    "    # 한 페이지당 큰 제목 기사 10개씩\n",
    "    for start in range(1,52,10) :\n",
    "        site = f'https://search.naver.com/search.naver?where=news&query={query1}%26{query2}%26{query3}&sm=tab_opt&sort=0&photo=0&field=0&reporter_article=&pd=3&ds={ds}&de={de}&docid=&nso=so%3Ar%2Cp%3Afrom20190101to20200206%2Ca%3Aall&mynews=0&&start={start}&related=0'\n",
    "        driver.get(site)\n",
    "        \n",
    "        time.sleep(1)\n",
    "        # 한 페이지당 네이버뉴스 링크가 있는 기사의 태그번호를 끝까지 센다.\n",
    "        # ul_tag = driver.find_element_by_css_selector('#main_pack > div > ul')\n",
    "        # li_list = ul_tag.find_elements_by_css_selector('li')\n",
    "        init = 1\n",
    "        end = (int(init//10)+1)*15\n",
    "        \n",
    "        for i in range(init, end+1, 1) :\n",
    "            # 기사제목을 가져온다. (sp_nws{i}가 없으면 다음 기사 탐색)\n",
    "            try :\n",
    "                li_tag = driver.find_element_by_css_selector(f'#sp_nws{i}')\n",
    "                a_list = li_tag.find_element_by_css_selector('dl > dt > a')\n",
    "\n",
    "                # 네이버뉴스 링크가 있으면 \n",
    "                # 딕셔너리에 기사제목과 링크주소 추가 후\n",
    "                # 기사를 열어본다.\n",
    "                news_link = driver.find_element_by_css_selector(f'#sp_nws{i} > dl > dd.txt_inline > a')\n",
    "                if news_link.text == '네이버뉴스':\n",
    "                    article_dict['title'].append(a_list.text.strip(''))\n",
    "                    article_dict['link'].append(news_link.get_attribute('href'))\n",
    "                    news_link.click()\n",
    "                \n",
    "                    # 네이버뉴스가 열린 탭으로 이동\n",
    "                    last_tab = driver.window_handles[-1]\n",
    "                    driver.switch_to.window(window_name=last_tab)\n",
    "\n",
    "                    # 좋아요 수와 화나요 수를 가져온다.\n",
    "                    like = driver.find_element_by_css_selector('#spiLayer > div._reactionModule.u_likeit > ul > li.u_likeit_list.good > a > span.u_likeit_list_count._count')\n",
    "                    angry = driver.find_element_by_css_selector('#spiLayer > div._reactionModule.u_likeit > ul > li.u_likeit_list.angry > a > span.u_likeit_list_count._count')\n",
    "                    article_dict['like'].append(int(like.text))\n",
    "                    article_dict['angry'].append(int(angry.text))\n",
    "\n",
    "                    # 기사 탭을 닫고 검색결과 화면으로 돌아간다.\n",
    "                    driver.close()\n",
    "                    first_tab = driver.window_handles[0]\n",
    "                    driver.switch_to.window(window_name=first_tab)\n",
    "                \n",
    "                # 네이버뉴스 링크 없으면 패스\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "            # sp_nws{i}가 없는 경우 패스 (예외처리)\n",
    "            except :\n",
    "                pass\n",
    "\n",
    "        # 내부for문 종료\n",
    "        init += 10\n",
    "    # 외부 for문 종료\n",
    "    \n",
    "    # 데이터프레임으로 추가\n",
    "    df = pd.DataFrame.from_dict(article_dict)\n",
    "    df.to_csv('naver_article.csv', encoding='utf-8-sig', index=False)\n",
    "    print('저장완료')\n",
    "    \n",
    "        # 한 페이지의 기사를 모두 탐색했으면 다음 페이지로 넘어간다.\n",
    "        # next_page_btn = driver.find_element_by_css_selector('#main_pack > div > div.paging > a.next')\n",
    "        # next_page_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "# 네이버뉴스를 검색하는 함수\n",
    "def article_search():\n",
    "    # 검색어, 기간을 입력한다.\n",
    "    # 날짜 형식 : yyyy.mm.dd\n",
    "    query1 = input('키워드1 : ')\n",
    "    query2 = input('키워드2 : ')\n",
    "    query3 = input('키워드3 : ')\n",
    "    ds = input('시작일 : ')\n",
    "    de = input('종료일 : ')\n",
    "    \n",
    "    # 기사 저장하는 함수 호출\n",
    "    article_save(query1, query2, query3, ds, de)\n",
    "        \n",
    "    # 기사 수집이 끝나면 '작업완료' 메시지 출력 후 프로그램 종료    \n",
    "    return '작업완료'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "키워드1 : 강남구\n",
      "키워드2 : 재개발\n",
      "키워드3 : 재건축\n",
      "시작일 : 2018.01.01\n",
      "종료일 : 2019.01.01\n",
      "저장완료\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'작업완료'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
